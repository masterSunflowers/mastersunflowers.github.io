---
title: Support Vector Machine
author: Thieu Luu
date: 2025-01-30
category: machine-learning
layout: post
---

# Bài toán Support Vector Machine

- Cho 2 class được gán nhãn, tìm một đường (dữ liệu 2 chiều), một mặt phẳng (dữ liệu 3 chiều) hoặc một siêu phẳng - hyperplane (dữ liệu nhiều chiều), sau đây gọi chung là đường phẳng, để chia toàn bộ các điểm dữ liệu thuộc class 1 về một phía và toàn bộ các điểm dữ liệu thuộc class 2 về một phía và đường phẳng đó là tốt nhất, nghĩa là khoảng cách từ điểm gần nhất đến đường phẳng là lớn nhất. Giả định rằng tồn tại một đường phẳng như thế.
- Nếu như các điểm dữ liệu không chia tuyến tính được trên không gian của các điểm dữ liệu $\mathbb{R}^n$ nhưng lại có thể chia tuyến tính ở không gian dữ liệu có chiều lớn hơn $\mathbb{R}^k$ với một hàm biến đổi tương ứng từng điểm dữ liệu $\textbf{x}^{(i)} \rightarrow \phi(\textbf{x}^{i})$

# Maximum Margin Classifier

## Kí hiệu

Nhãn $y^{(i)} \in \{-1, +1\}$
Output $\hat{y}^{(i)} \in \{-1, +1\}$

$$
g(z^{(i)}) = 
\begin{cases}
1 \quad if \quad z^{(i)} >= 0 \\
-1 \quad otherwise
\end{cases}
$$

$\hat{y}^{(i)} = g(\textbf{w}^T\textbf{x}^{(i)} + b), (\textbf{x}^{(i)}, \textbf{w} \in \mathbb{R}^{n})$

## Khoảng cách hình học

Khoảng cách từ điểm $\textbf{x}_0=(x_1, x_2, ..., x_n)$ đến đường thẳng (mặt phẳng, siêu phẳng) $P: \textbf{w}^T\textbf{x} + b= 0$ là:

$$
    d(\textbf{x}_0, P) = \frac{|\textbf{w}^T\textbf{x}_0 + b|}{\sqrt{\textbf{w}^T\textbf{w}}} = \frac{|\textbf{w}^T\textbf{x}_0 + b|}{||\textbf{w}||}
$$

## Mô hình hóa bài toán

Bài toán này sẽ cần đảm bảo 2 điều kiện:

1. Các điểm dữ liệu được phân loại đúng theo nhãn ban đầu của chúng, nghĩa là các điểm dữ liệu có cùng nhãn sẽ thuộc 1 phía so với đường phẳng phân loại.
2. Đường phẳng phân loại cần thỏa mãn khoảng cách từ điểm gần nhất của tập điểm dữ liệu so với đường phẳng này là lớn nhất.

Để  làm được việc đó, với $y^{(i)} = 1$ ta mong muốn $\frac{\textbf{w}^T\textbf{x}^{(i)} + b}{||\textbf{w}||} >> 0$ và ngược lại, với $y^{(i)} = -1$ ta muốn $\frac{\textbf{w}^T\textbf{x}^{(i)} + b}{||\textbf{w}||} << 0$, tổng quát, chúng ta cần $f^{(i)} = y^{(i)}\frac{(\textbf{w}^T\textbf{x}^{(i)} + b)}{||\textbf{w}||} >> 0$.
Giả sử $f = min_if^{(i)}$ suy ra $\forall i \in \{1, 2, ..., n\}$ ta có $f^{(i)} \ge f$. Bài toán trở thành tìm cực đại của $f$ thỏa mãn điều kiện trên.

$$
    argmax_{f,\textbf{w},b}f \quad subject \space to \quad \forall i\in\{1, 2, ..., n\}: \space y^{(i)}\frac{\textbf{w}^T\textbf{x}^{(i)} + b}{||\textbf{w}||} \ge f
$$

Ta có nhận xét rằng, nếu ta thay hệ số $$\textbf{w}$$ bằng $$k\textbf{w}$$ và $$b$$ bằng $$kb$$ thì đường phẳng là không đổi, vậy nếu ra ràng buộc $$\|\textbf{w}\| = \frac{1}{f}$$ thì bài toán trở thành:

$$
    argmax_{\textbf{w},b}\frac{1}{||\textbf{w}||} \quad subject \space to \quad \forall i\in\{1, 2, ..., n\}: \space y^{(i)}(\textbf{w}^T\textbf{x}^{(i)} + b) \ge 1
$$

Ta có thể dễ dàng biến đổi công thức thành:

$$
    argmin_{\textbf{w},b}\frac{1}{2}||\textbf{w}||^2 \quad subject \space to \quad \forall i\in\{1, 2, ..., n\}: \space 1 - y^{(i)}(\textbf{w}^T\textbf{x}^{(i)} + b) \le 0
$$

## Áp dụng bài toán cực tiểu hóa có ràng buộc bất đẳng thức

Cực tiểu hóa $f(\textbf{x})$, với các điều kiện $g_i(\textbf{x}) \le 0 \space(i=1, 2,...,r)$
=> Điều kiện cần để $\textbf{x}_0$ là một lời giải:

$$
\begin{cases}
\frac{\partial}{\partial \textbf{x}}(f(\textbf{x}) + \sum_{i=1}^r \alpha_ig_i(\textbf{x}))|_{\textbf{x}=\textbf{x}_0} = 0\\
g_i(\textbf{x}) \le 0
\end{cases} \quad with \quad \alpha_i \ge 0
$$

Hàm Lagrange:

$$
L = f(\textbf{x}) + \sum_{i=1}^{r} \alpha_ig_i(\textbf{x})
$$

Biểu thức Lagrange đối với bài toán:

$$
\begin{equation}
L_P(\textbf{w}, b, \boldsymbol\alpha) = \frac{1}{2}\langle \textbf{w} \cdot \textbf{w}\rangle + \sum_{i =1}^r \alpha_i(1 - y^{(i)}(\langle \textbf{w} \cdot \textbf{x}^{(i)} \rangle + b)) \\
\end{equation}
$$

trong đó $\alpha_i (\ge 0)$ là các hệ số nhân Lagrange

Lý thuyết tối ưu chỉ ra rằng một lời giải tối ưu cho biểu thức trên phải thỏa mãn các điều kiện nhất định, được gọi là **các điều kiện Karush-Kuhn-Tucker** (là các điều kiện cần, nhưng không phải là các điều kiện đủ):

Hai điều kiện ban đầu của bài toán tối ưu:

$$
\begin{equation}
y^{(i)}(\langle \textbf{w} \cdot \textbf{x}^{(i)} \rangle + b) - 1 \ge 0, \forall \textbf{x}^{(i)}(i=1,2,...,r)
\end{equation}
$$

$$
\begin{equation}
\alpha_i \ge 0
\end{equation}
$$

Đạo hàm bộ phận của biểu thức Lagrange phải bằng 0:

$$
\begin{equation}
\frac{\partial L_p}{\partial \textbf{w}} = \textbf{w} - \sum_{i=1}^r \alpha_i y^{(i)}\textbf{x}^{(i)} = 0 
\end{equation}
$$

$$
\begin{equation}
\frac{\partial L_p}{\partial b} = -\sum_{i=1}^r \alpha_iy^{(i)} = 0
\end{equation}
$$

$$
\begin{equation}
\alpha_i(y^{(i)}(\langle \textbf{w} \cdot \textbf{x}^{(i)} \rangle + b) - 1) = 0
\end{equation}
$$

Điều kiện bổ sung (6) chỉ ra những ví dụ (điểm dữ liệu) thuộc các mặt siêu phẳng lề mới có $\alpha_i \gt 0$ bởi vì ở những ví dụ này thì $y^{(i)}(\langle \textbf{w} \cdot \textbf{x}^{(i)} \rangle + b) - 1 = 0$ -> Những điểm dữ liệu này được gọi là **các vector hỗ trợ**. Đối với các điểm dữ liệu khác thì $\alpha_i = 0$

Trong trường hợp tổng quát, các điều kiện Karush-Kuhn-Tucker là điều kiện cần đối với một lời giải tối ưu, nhưng chưa đủ.

**Tuy nhiên đối với SVM, bài toán cực tiểu hóa có hàm mục tiêu lồi (convex) và các ràng buộc tuyến tính, thì các điều kiện Karush-Kuhn-Tucker là điều kiện cần và đủ cho một lời giải tối ưu.**

Giải quyết bài toán tối ưu này vẫn là một nhiệm vụ khó, do sự tồn tại của các ràng buộc bất đẳng thức. Phương pháp Lagrange giải quyết bài toán tối ưu hàm lồi dẫn đến một **bài toán đối ngẫu (dual)** của bài toán tối ưu. Dễ hơn so với **bài toán tối ưu ban đầu (primal)**

Để thu được bài toán đối ngẫu từ biểu thức ban đầu cần:

1. Gán giá trị bằng 0 đối với các đạo hàm bộ phận của biểu thức Lagrange trong (1) đối với các biến ban đầu ($\textbf{w}$ và b)
2. Áp dụng các quan hệ thu được đối với biểu thức Lagrange

**Biểu thức đối ngẫu $L_D$**

$$
\begin{equation}
L_D(\boldsymbol\alpha) = \sum_{i=1}^r \alpha_i - \frac{1}{2}\sum_{i=1}^r\sum_{j=1}^r \alpha_i \alpha_j y^{(i)}y^{(j)}\langle \textbf{x}^{(i)} \cdot \textbf{x}^{(j)}\rangle
\end{equation}
$$

Cả hai biểu thức LP và LD đều là các biểu thức Lagrange

- Dựa trên cùng một hàm một tiêu – nhưng với các ràng buộc khác nhau
- Lời giải tìm được, bằng cách cực tiểu hóa LP hoặc cực đại hóa LD

## Bài toán tối ưu đối ngẫu

Cực đại hóa hàm mục tiêu:

$$
\begin{equation}
L_D(\boldsymbol\alpha) = \sum_{i=1}^r \alpha_i - \frac{1}{2}\sum_{i=1}^r\sum_{j=1}^r \alpha_i \alpha_j y^{(i)}y^{(j)}\langle \textbf{x}^{(i)} \cdot \textbf{x}^{(j)}\rangle
\end{equation}
$$

Với điều kiện:

$$
\begin{cases}
\sum_{i=1}^r \alpha_i y^{(i)} = 0\\
\alpha_i \ge 0, \forall i=1,...r
    \end{cases} 
$$

Đối với hàm mục tiêu là hàm lồi và các ràng buộc tuyến tính, giá trị cực đại của $L_D$ xảy ra tại cùng các giá trị của $\textbf{w}$, $b$ và $\alpha_i$ giúp đạt được giá trị cực tiểu của $L_P$

- Giải bài toán (8), ta thu được các hệ số nhân Lagrange $\alpha_i$ (các hệ số $\alpha_i$ này sẽ được dùng để tính $\textbf{w}$ và $b$)
- Giải bài toán (8) cần đến các phương pháp lặp (để giải quyết bài  toán tối ưu hàm lồi bậc hai có các ràng buộc tuyến tính)

## Tính giá trị w*và b*

$$
\textbf{w}^{*} = \sum_{i=1}^r \alpha_i y^{(i)} \textbf{x}^{(i)} = \sum_{\textbf{x}^{(i)} \in SV} \alpha_i y^{(i)} \textbf{x}^{(i)} \quad (SV \space is \space set \space of \space Support \space Vectors)
$$

Với $\textbf{x}^{(k)}$ là một điểm dữ liệu bất kỳ thuộc SV ta có

$$
y^{(k)}(\langle \textbf{w}^{*} \cdot \textbf{x}^{(k)} \rangle + b^{*}) = 1 \Rightarrow b^{*} = y^{(k)} - \langle \textbf{w}^{*} \cdot \textbf{x}^{(k)} \rangle
$$

## Phân lớp cho ví dụ mới

- Ranh giới quyết định phân lớp được xác định bởi đường phẳng:

$$
\begin{equation}
f(\textbf{x}) = \langle \textbf{w}^{*} \cdot \textbf{x} \rangle + b^{*} = \sum_{\textbf{x}^{(i)} \in SV} \alpha_i y^{(i)} \langle \textbf{x}^{(i)} \cdot \textbf{x} \rangle + b^{*} = 0 
\end{equation}
$$

- Đối với một ví dụ (điểm dữ liệu) mới cần phân lớp $\textbf{z}$ ta tính:

$$
\begin{equation}
sign(\langle \textbf{w}^{*} \cdot \textbf{z} \rangle + b^{*}) = \sum_{\textbf{x}^{(i)} \in SV} \alpha_i y^{(i)} \langle \textbf{x}^{(i)} \cdot \textbf{z} \rangle + b^{*}
\end{equation}
$$
